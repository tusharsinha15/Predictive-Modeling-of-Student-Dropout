# -*- coding: utf-8 -*-
"""BDA RP.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CbSS1CalBWbVMAWJPB_OqOQfEtIKQz7n
"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np

# 1. Load the dataset
try:
    df = pd.read_csv('/content/JEE_Dropout_After_Class_12.csv')
except FileNotFoundError:
    print("Error: The file 'JEE_Dropout_After_Class_12.csv' was not found. Please upload the file or check the path.")
    exit()

# Display the first few rows and column information
print("Initial Data Overview:")
print(df.head())
print("\nColumn Information:")
df.info()

# 2. Preprocessing: Handle categorical features and split data
# The 'dropout' column is already in the correct format (1 or 0)
categorical_cols = df.select_dtypes(include=['object']).columns.tolist()

# Use one-hot encoding for categorical variables
df_encoded = pd.get_dummies(df, columns=categorical_cols, drop_first=True)

# Define features (X) and target (y)
X = df_encoded.drop('dropout', axis=1)
y = df_encoded['dropout']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)
print(f"\nData split into {len(X_train)} training samples and {len(X_test)} testing samples.")
print(f"Dropout ratio in training set: {y_train.sum() / len(y_train):.2f}")
print(f"Dropout ratio in testing set: {y_test.sum() / len(y_test):.2f}")

# Scale numerical features for better model performance (especially for Logistic Regression)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)


# 3. Model Implementation and Comparison
print("\n--- 3. Implementing and Comparing Models ---")

# Initialize models
models = {
    "Logistic Regression": LogisticRegression(random_state=42, max_iter=1000),
    "Random Forest": RandomForestClassifier(random_state=42),
    "Gradient Boosting": GradientBoostingClassifier(random_state=42)
}

results = {}

for name, model in models.items():
    print(f"\nTraining {name}...")
    # Logistic Regression requires scaled data
    if name == "Logistic Regression":
        model.fit(X_train_scaled, y_train)
        y_pred = model.predict(X_test_scaled)
    else:
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)

    # Evaluate model performance
    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred)
    recall = recall_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)

    results[name] = {
        "Accuracy": accuracy,
        "Precision": precision,
        "Recall": recall,
        "F1-Score": f1
    }

    print(f"{name} Performance:")
    print(f"  Accuracy: {accuracy:.4f}")
    print(f"  Precision: {precision:.4f}")
    print(f"  Recall: {recall:.4f}")
    print(f"  F1-Score: {f1:.4f}")

    # Generate and display a confusion matrix for each model
    cm = confusion_matrix(y_test, y_pred)
    plt.figure(figsize=(6, 4))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['No Dropout', 'Dropout'], yticklabels=['No Dropout', 'Dropout'])
    plt.title(f'Confusion Matrix for {name}')
    plt.xlabel('Predicted Label')
    plt.ylabel('True Label')
    plt.show()

# 4. Feature Importance Analysis (for tree-based models)
print("\n--- 4. Feature Importance ---")
# Feature importance is not directly available for Logistic Regression in the same way,
# but we can look at the coefficients.
print("Random Forest Feature Importance:")
feature_importances_rf = pd.DataFrame({
    'feature': X.columns,
    'importance': models['Random Forest'].feature_importances_
}).sort_values('importance', ascending=False)
print(feature_importances_rf.head(10))

print("\nGradient Boosting Feature Importance:")
feature_importances_gb = pd.DataFrame({
    'feature': X.columns,
    'importance': models['Gradient Boosting'].feature_importances_
}).sort_values('importance', ascending=False)
print(feature_importances_gb.head(10))

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
# Make sure the file 'JEE_Dropout_After_Class_12.csv' is in your working directory.
df = pd.read_csv('/content/JEE_Dropout_After_Class_12.csv')

# --- Plot 1: Bar chart for Dropout vs. No Dropout ---
plt.figure(figsize=(8, 6))
sns.countplot(x='dropout', data=df)
plt.title('Count of Dropout vs. No Dropout Students')
plt.xlabel('Dropout (0 = No, 1 = Yes)')
plt.ylabel('Number of Students')
plt.xticks([0, 1], ['No Dropout', 'Dropout'])
plt.savefig('dropout_count_bar_chart.png')
plt.show()

# --- Plot 2: Box plot for Daily Study Hours vs. Dropout ---
plt.figure(figsize=(10, 6))
sns.boxplot(x='dropout', y='daily_study_hours', data=df)
plt.title('Daily Study Hours by Dropout Status')
plt.xlabel('Dropout (0 = No, 1 = Yes)')
plt.ylabel('Daily Study Hours')
plt.xticks([0, 1], ['No Dropout', 'Dropout'])
plt.savefig('daily_study_hours_boxplot.png')
plt.show()

# --- Plot 3: Stacked Bar Chart for Family Income vs. Dropout ---
# Create a cross-tabulation for Family Income and Dropout
family_income_dropout = pd.crosstab(df['family_income'], df['dropout'], normalize='index') * 100

family_income_dropout.plot(kind='bar', stacked=True, figsize=(10, 7))
plt.title('Dropout Percentage by Family Income')
plt.xlabel('Family Income')
plt.ylabel('Percentage')
plt.xticks(rotation=0)
plt.legend(title='Dropout', labels=['No Dropout', 'Dropout'])
plt.savefig('family_income_stacked_bar_chart.png')
plt.show()